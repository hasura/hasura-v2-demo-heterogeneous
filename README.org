# -*- mode: org; -*-

#+STARTUP: indent

* What

This project comprises instructions for setting up heterogeneous data
sources with Hasura v2.

* Why

There can never be too many tutorials, walk-throughs, and lighted
pathways for setting up Hasura.  This is yet another one.

* How

This project uses Docker Compose to launch services for PostgreSQL,
for MongoDB, for Redis, for Hasura, and for a Hasura Data Connector.
It also relies on a handful of environment variables to be supplied by
the user.  As a tutorial, it is divided into two parts:  Part A and
Part B.

Part A offers a sequence of steps to be performed at the Command Line
and optionally in a text editor, to create a Docker Compose file and
to acquire supporting initialization files to create the services.

Part B offers a sequence of steps to be performed in Hasura Console
once all the services have been launched.

* Part A:  At the Command Line

** Step 1:  Create a new directory.

Create a directory to work in and move to it.

#+begin_src bash :eval never-export :exports code :session scratch :results none
  rm -rf scratch
  mkdir -p scratch
  cd scratch
#+end_src

- What did this do? ::
  This step just creates a scratch workspace for the project.

** Step 2:  Create a PostgreSQL initialization directory.

Create a directory to mount into the PostgreSQL container in order to
initialize the database.

#+begin_src bash :eval never-export :exports code :session scratch :results none
  mkdir -p initdb.d-postgres
#+end_src

- What did this do? ::
  This step creates a directory that will be mounted into the
  PostgreSQL container as a volume, in a special directory that the
  container image uses to access initialization files.

** Step 3:  Download the PostgreSQL initialization files.

Download PostgreSQL initialization scripts into its initialization
directory.

#+begin_src bash :eval never-export :exports code :session scratch :results none
  wget -O initdb.d-postgres/03_chinook_database.sql https://raw.githubusercontent.com/hasura/hasura-v2-demo-heterogeneous/main/initdb.d-postgres/03_chinook_database.sql
  wget -O initdb.d-postgres/04_chinook_ddl.sql https://raw.githubusercontent.com/hasura/hasura-v2-demo-heterogeneous/main/initdb.d-postgres/04_chinook_ddl.sql
  wget -O initdb.d-postgres/05_chinook_dml.sql https://raw.githubusercontent.com/hasura/hasura-v2-demo-heterogeneous/main/initdb.d-postgres/05_chinook_dml.sql
#+end_src

- What did this do? ::
  This step downloaded PostgreSQL SQL initialization files from this
  GitHub repository, with DDL and DML for the Chinook sample database.

** Step 4:  Scaffold the Docker Compose file.

Use a code editor to start the Docker Compose file with its preamble.

#+begin_src yaml
version: '3.1'
services:
#+end_src

Alternatively, add to the file from the command line.

#+begin_src bash :eval never-export :exports code :session scratch :results none
cat <<'EOF' > docker-compose.yaml
version: '3.1'
services:
EOF
#+end_src

- What did this do? ::
  This step added the Docker Compose preamble to the
  ~docker-compose.yaml~ file to set the version and create the
  ~services~ node.

** Step 5:  Add the ~postgres~ service.

Use a code editor to add a stanza for the ~postgres~ service.

#+begin_src yaml
  postgres:
    image: postgres:16          # Use a modern version of PostgreSQL.
    environment:                # Set its superuser username and password.
      POSTGRES_PASSWORD: postgres
    volumes:                    # Initialize from the contents of the initialization directory.
      - ./initdb.d-postgres:/docker-entrypoint-initdb.d:ro
    healthcheck:                # Use a sensible healthcheck.
      test: psql -U postgres -d chinook -c "select count(*) from \"Artist\""
#+end_src

Alternatively, add to the file from the command line.

#+begin_src bash :eval never-export :exports code :session scratch :results none
cat <<'EOF' >> docker-compose.yaml
  postgres:
    image: postgres:16          # Use a modern version of PostgreSQL.
    environment:                # Set its superuser username and password.
      POSTGRES_PASSWORD: postgres
    volumes:                    # Initialize from the contents of the initialization directory.
      - ./initdb.d-postgres:/docker-entrypoint-initdb.d:ro
    healthcheck:                # Use a sensible healthcheck.
      test: psql -U postgres -d chinook -c "select count(*) from \"Artist\""
EOF
#+end_src

- What did this do? ::
  This step adds the ~postgres~ service.  PostgreSQL is used /both/
  as a Hasura data source /and/ as the Hasura metadata database.  In a
  more realistic setting, typically these will be different databases.
  In a tutorial, keeping them in one database is simpler.  The Hasura
  metadata database is largely of incidental importance for this
  tutorial, since its only role is as a channel for synchronizing
  metadata changes across a horizontally-scaled cluster of Hasura
  instances.  With only one instance, that obviously is irrelevant for
  this tutorial.  Nevertheless, the presence of a metadata database is
  a /requirement/ for Hasura v2 even to start.

** Step 7:  Test the PostgreSQL service.

Use Docker Compose to start the ~postgres~ service.

#+begin_src bash :eval never-export :exports code :session scratch :results none
  docker compose up -d postgres
#+end_src

Run a query against the database to verify that it has been
initialized.

#+begin_src bash :eval never-export :exports code :session scratch :results output
  docker exec scratch-postgres-1 psql -U postgres -d chinook -c "select count(*) from \"Artist\""
#+end_src

#+RESULTS:
: count
: -------
:    276
: (1 row)

- What did this do? ::
  This step launched the Docker Compose ~postgres~ service and ran a
  test query just to validate that it has been initialized properly.

** Step 8:  Create a MongoDB initialization directory.

Create a directory to mount into the MongoDB container in order to
initialize the database.

#+begin_src bash :eval never-export :exports code :session scratch :results none
  mkdir -p initdb.d-mongo
#+end_src

- What did this do? ::
  This step creates a directory that will be mounted into the MongoDB
  container as a volume, in a special directory that the container
  image uses to access initialization files.

** Step 9:  Download the MongoDB initialization files.

Download Mongo DB initialization files into its initialization
directory.

#+begin_src bash :eval never-export :exports code :session scratch :results none
  wget -O initdb.d-mongo/01_import_data.sh https://raw.githubusercontent.com/hasura/hasura-v2-demo-heterogeneous/main/initdb.d-mongo/01_import_data.sh
  wget -O initdb.d-mongo/postgres.Album.json https://raw.githubusercontent.com/hasura/hasura-v2-demo-heterogeneous/main/initdb.d-mongo/postgres.Album.json
  wget -O initdb.d-mongo/postgres.Artist.json https://raw.githubusercontent.com/hasura/hasura-v2-demo-heterogeneous/main/initdb.d-mongo/postgres.Artist.json
  wget -O initdb.d-mongo/postgres.Track.json https://raw.githubusercontent.com/hasura/hasura-v2-demo-heterogeneous/main/initdb.d-mongo/postgres.Track.json
#+end_src

- What did this do? ::
  This step downloaded MongoDB initialization scripts and related data
  files from this GitHub repository.

** Step 10:  Add the ~mongo~ service.

Use a code editor to add a stanza for the ~mongo~ service.

#+begin_src yaml
  mongo:
    image: mongo:6              # Use a modern version of MongoDB.
    environment:                # Set its superuser username and password.
      MONGO_INITDB_ROOT_PASSWORD: mongo
      MONGO_INITDB_ROOT_USERNAME: mongo
    volumes:                    # Initialize from the contents of the initialization directory.
      - ./initdb.d-mongo:/docker-entrypoint-initdb.d:ro
    depends_on:                 # Wait until postgres starts up first.
      postgres:
        condition: service_healthy
#+end_src

Alternatively, add to the file from the command line.

#+begin_src bash :eval never-export :exports code :session scratch :results none
cat <<'EOF' >> docker-compose.yaml
  mongo:
    image: mongo:6              # Use a modern version of MongoDB.
    environment:                # Set its superuser username and password.
      MONGO_INITDB_ROOT_PASSWORD: mongo
      MONGO_INITDB_ROOT_USERNAME: mongo
    volumes:                    # Initialize from the contents of the initialization directory.
      - ./initdb.d-mongo:/docker-entrypoint-initdb.d:ro
    depends_on:                 # Wait until postgres starts up first.
      postgres:
        condition: service_healthy
EOF
#+end_src

- What did this do? ::
  This step added a stanza for the ~mongo~ service to the Docker
  Compose file.

** Step 11:  Test the MongoDB service.

User Docker Compose to start the ~mongo~ service.

#+begin_src bash :eval never-export :exports code :session scratch :results none
  docker compose up -d mongo
#+end_src

Run a query against the database to verify that it has been
initialized.

#+begin_src bash :eval never-export :exports code :session scratch :results output
  docker exec scratch-mongo-1 mongosh --quiet -u mongo -p mongo --eval "db.postgres.Album.findOne()" admin
#+end_src

#+RESULTS:
#+begin_example
{"t":{"$date":"2024-07-31T16:11:44.295+00:00"},"s":"I",  "c":"NETWORK",  "id":22943,   "ctx":"listener","msg":"Connection accepted","attr":{"remote":"127.0.0.1:42454","uuid":"7016f115-1f92-4233-86db-6f2590d63450","connectionId":5,"connectionCount":1}}
{"t":{"$date":"2024-07-31T16:11:44.299+00:00"},"s":"I",  "c":"NETWORK",  "id":51800,   "ctx":"conn5","msg":"client metadata","attr":{"remote":"127.0.0.1:42454","client":"conn5","negotiatedCompressors":[],"doc":{"application":{"name":"mongosh 2.2.10"},"driver":{"name":"nodejs|mongosh","version":"6.7.0|2.2.10"},"platform":"Node.js v20.12.2, LE","os":{"name":"linux","architecture":"x64","version":"3.10.0-327.22.2.el7.x86_64","type":"Linux"},"env":{"container":{"runtime":"docker"}}}}}
{"t":{"$date":"2024-07-31T16:11:44.302+00:00"},"s":"I",  "c":"NETWORK",  "id":22943,   "ctx":"listener","msg":"Connection accepted","attr":{"remote":"127.0.0.1:42464","uuid":"848d583f-9ca6-4b7a-a288-31101cfe3f3a","connectionId":6,"connectionCount":2}}
{"t":{"$date":"2024-07-31T16:11:44.303+00:00"},"s":"I",  "c":"NETWORK",  "id":51800,   "ctx":"conn6","msg":"client metadata","attr":{"remote":"127.0.0.1:42464","client":"conn6","negotiatedCompressors":[],"doc":{"application":{"name":"mongosh 2.2.10"},"driver":{"name":"nodejs|mongosh","version":"6.7.0|2.2.10"},"platform":"Node.js v20.12.2, LE","os":{"name":"linux","architecture":"x64","version":"3.10.0-327.22.2.el7.x86_64","type":"Linux"},"env":{"container":{"runtime":"docker"}}}}}
{"t":{"$date":"2024-07-31T16:11:44.308+00:00"},"s":"I",  "c":"ACCESS",   "id":20250,   "ctx":"conn6","msg":"Authentication succeeded","attr":{"mechanism":"SCRAM-SHA-256","speculative":true,"principalName":"mongo","authenticationDatabase":"admin","remote":"127.0.0.1:42464","extraInfo":{}}}
{"t":{"$date":"2024-07-31T16:11:44.348+00:00"},"s":"I",  "c":"NETWORK",  "id":22943,   "ctx":"listener","msg":"Connection accepted","attr":{"remote":"127.0.0.1:42466","uuid":"9aa0c2af-d600-4be6-bef6-48d03bc31985","connectionId":7,"connectionCount":3}}
{"t":{"$date":"2024-07-31T16:11:44.348+00:00"},"s":"I",  "c":"NETWORK",  "id":22943,   "ctx":"listener","msg":"Connection accepted","attr":{"remote":"127.0.0.1:42470","uuid":"ef6cc7b1-ccc4-4a6e-8d07-641d5d742b4b","connectionId":8,"connectionCount":4}}
{"t":{"$date":"2024-07-31T16:11:44.352+00:00"},"s":"I",  "c":"NETWORK",  "id":51800,   "ctx":"conn7","msg":"client metadata","attr":{"remote":"127.0.0.1:42466","client":"conn7","negotiatedCompressors":[],"doc":{"application":{"name":"mongosh 2.2.10"},"driver":{"name":"nodejs|mongosh","version":"6.7.0|2.2.10"},"platform":"Node.js v20.12.2, LE","os":{"name":"linux","architecture":"x64","version":"3.10.0-327.22.2.el7.x86_64","type":"Linux"},"env":{"container":{"runtime":"docker"}}}}}
{"t":{"$date":"2024-07-31T16:11:44.352+00:00"},"s":"I",  "c":"NETWORK",  "id":51800,   "ctx":"conn8","msg":"client metadata","attr":{"remote":"127.0.0.1:42470","client":"conn8","negotiatedCompressors":[],"doc":{"application":{"name":"mongosh 2.2.10"},"driver":{"name":"nodejs|mongosh","version":"6.7.0|2.2.10"},"platform":"Node.js v20.12.2, LE","os":{"name":"linux","architecture":"x64","version":"3.10.0-327.22.2.el7.x86_64","type":"Linux"},"env":{"container":{"runtime":"docker"}}}}}
{"t":{"$date":"2024-07-31T16:11:44.353+00:00"},"s":"I",  "c":"ACCESS",   "id":20250,   "ctx":"conn7","msg":"Authentication succeeded","attr":{"mechanism":"SCRAM-SHA-256","speculative":true,"principalName":"mongo","authenticationDatabase":"admin","remote":"127.0.0.1:42466","extraInfo":{}}}
{"t":{"$date":"2024-07-31T16:11:44.354+00:00"},"s":"I",  "c":"ACCESS",   "id":20250,   "ctx":"conn8","msg":"Authentication succeeded","attr":{"mechanism":"SCRAM-SHA-256","speculative":true,"principalName":"mongo","authenticationDatabase":"admin","remote":"127.0.0.1:42470","extraInfo":{}}}
{
  _id: ObjectId('6637f6cc7cda30b626bb1d07'),
  AlbumId: 1,
  Title: 'For Those About To Rock We Salute You',
  ArtistId: 1
}
{"t":{"$date":"2024-07-31T16:11:44.365+00:00"},"s":"I",  "c":"NETWORK",  "id":22944,   "ctx":"conn8","msg":"Connection ended","attr":{"remote":"127.0.0.1:42470","uuid":"ef6cc7b1-ccc4-4a6e-8d07-641d5d742b4b","connectionId":8,"connectionCount":3}}
{"t":{"$date":"2024-07-31T16:11:44.365+00:00"},"s":"I",  "c":"NETWORK",  "id":22944,   "ctx":"conn6","msg":"Connection ended","attr":{"remote":"127.0.0.1:42464","uuid":"848d583f-9ca6-4b7a-a288-31101cfe3f3a","connectionId":6,"connectionCount":2}}
{"t":{"$date":"2024-07-31T16:11:44.365+00:00"},"s":"I",  "c":"NETWORK",  "id":22944,   "ctx":"conn5","msg":"Connection ended","attr":{"remote":"127.0.0.1:42454","uuid":"7016f115-1f92-4233-86db-6f2590d63450","connectionId":5,"connectionCount":1}}
{"t":{"$date":"2024-07-31T16:11:44.365+00:00"},"s":"I",  "c":"NETWORK",  "id":22944,   "ctx":"conn7","msg":"Connection ended","attr":{"remote":"127.0.0.1:42466","uuid":"9aa0c2af-d600-4be6-bef6-48d03bc31985","connectionId":7,"connectionCount":0}}
#+end_example

- What did this do? ::
  This step used the ~mongosh~ shell to execute a simple query against
  the ~mongo~ service, to check that it has been initialized properly.

** Step 12:  Add the ~mongo_data_connector~ service.

Use a code editor to add a stanza for the ~mongo-data-connector~
service.

#+begin_src yaml
  mongo_data_connector:         # Start the connector agent.
    image: hasura/mongo-data-connector:v2.38.0
    depends_on:                 # Wait until mongo starts up first.
      - mongo
#+end_src

Alternatively, add to the file from the command line.

#+begin_src bash :eval never-export :exports code :session scratch :results none
cat <<'EOF' >> docker-compose.yaml
  mongo_data_connector:         # Start the connector agent.
    image: hasura/mongo-data-connector:v2.38.0
    depends_on:                 # Wait until mongo starts up first.
      - mongo
EOF
#+end_src

- What did this do? ::
  This step added a MongoDB connector service to the Docker Compose
  file.  Hasura uses an independent connector agent for certain
  databases, such as MongoDB.

** Step 13:  Add the ~redis~ service.

Use a code editor to add a stanza for the ~redis~ service.

#+begin_src yaml
  redis:
    image: redis:latest
#+end_src

Alternatively, add to the file from the command line.

#+begin_src bash :eval never-export :exports code :session scratch :results none
cat <<'EOF' >> docker-compose.yaml
  redis:
    image: redis:latest
EOF
#+end_src

- What did this do? ::
  This step added a Redis service to the Docker Compose file.  Hasura
  EE uses Redis in two ways.  First, Redis is used for caching.
  Second, Redis is used to store counters and other data that are used
  by Hasura security features like rate-limiting.

** Step 14:  Add Hasura.

Use a code editor to add a stanza for the ~hasura~ service.

#+begin_src yaml
  hasura:                       # Start Hasura.
    image: hasura/graphql-engine:v2.40.0
    depends_on:                 # Wait until the connector agent starts up first.
      - mongo_data_connector
    ports:                      # Expose it on a port taken from an environment variable
      - ${HGPORT}:8080
    healthcheck:                # Use a sensible healthcheck.
      test: curl -s http://localhost:8080/healthz
      start_period: 60s
    environment:                # Configure Hasura.
      HASURA_GRAPHQL_ADMIN_SECRET: hasura # Hasura EE requires an admin secret.
      HASURA_GRAPHQL_DEV_MODE: true       # We require dev mode.
      HASURA_GRAPHQL_EE_LICENSE_KEY: ${HASURA_GRAPHQL_EE_LICENSE_KEY} # Hasura EE requires a license key.
      HASURA_GRAPHQL_ENABLE_CONSOLE: true # We require Hasura Console.
      HASURA_GRAPHQL_MAX_CACHE_SIZE: 200  # Set Redis cache size.
      HASURA_GRAPHQL_METADATA_DATABASE_URL: postgres://postgres:postgres@postgres/metadata # Hasura requires a PostgreSQL DB for metadata.
      HASURA_GRAPHQL_METADATA_DEFAULTS: '{"backend_configs":{"dataconnector":{"Mongo":{"uri":"http://mongo_data_connector:3000"}}}}' # Tell Hasura about the connector agent.
      HASURA_GRAPHQL_RATE_LIMIT_REDIS_URL: redis://redis:6379 # Set the Redis URL for rate-limiting.
      HASURA_GRAPHQL_REDIS_URL: redis://redis:6379            # Use the same Redis URL for caching.
#+end_src

Alternatively, add to the file from the command line.

#+begin_src bash :eval never-export :exports code :session scratch :results none
cat <<'EOF' >> docker-compose.yaml
  hasura:                       # Start Hasura.
    image: hasura/graphql-engine:v2.40.0
    depends_on:                 # Wait until the connector agent starts up first.
      - mongo_data_connector
    ports:                      # Expose it on a port taken from an environment variable
      - ${HGPORT}:8080
    healthcheck:                # Use a sensible healthcheck.
      test: curl -s http://localhost:8080/healthz
      start_period: 60s
    environment:                # Configure Hasura.
      HASURA_GRAPHQL_ADMIN_SECRET: hasura # Hasura EE requires an admin secret.
      HASURA_GRAPHQL_DEV_MODE: true       # We require dev mode.
      HASURA_GRAPHQL_EE_LICENSE_KEY: ${HASURA_GRAPHQL_EE_LICENSE_KEY} # Hasura EE requires a license key.
      HASURA_GRAPHQL_ENABLE_CONSOLE: true # We require Hasura Console.
      HASURA_GRAPHQL_MAX_CACHE_SIZE: 200  # Set Redis cache size.
      HASURA_GRAPHQL_METADATA_DATABASE_URL: postgres://postgres:postgres@postgres/metadata # Hasura requires a PostgreSQL DB for metadata.
      HASURA_GRAPHQL_METADATA_DEFAULTS: '{"backend_configs":{"dataconnector":{"Mongo":{"uri":"http://mongo_data_connector:3000"}}}}' # Tell Hasura about the connector agent.
      HASURA_GRAPHQL_RATE_LIMIT_REDIS_URL: redis://redis:6379 # Set the Redis URL for rate-limiting.
      HASURA_GRAPHQL_REDIS_URL: redis://redis:6379            # Use the same Redis URL for caching.
EOF
#+end_src

- What did this do? ::
  This step added a service to the Docker Compose file for ~hasura~.

** Step 15:  Set environment variables.

Set environment variables to be used by Docker Compose but which
should not be hard-coded into the Docker Compose file

#+begin_src bash :eval never-export :exports code :session scratch :results none
  export HASURA_GRAPHQL_EE_LICENSE_KEY=<your EE license key>
  export HGPORT=8081		# or your own port
#+end_src

- What did this do? ::
  This step set the two environment variables that are actually
  necessary.

  - ~HASURA_GRAPHQL_EE_LICENSE_KEY~ :: Because this tutorial uses
    Enterprise features like Redis caching and the MongoDB connector
    agent, we need to use the Hasura EE version with a valid license
    key.
  - ~HGPORT~ :: Because we need to use Hasura Console in Part B of
    this tutorial, we need to access both it and the ~graphql-engine~
    instance within the container.

** Step 16:  Start the ~mongo_data_connector~, ~redis~ and ~hasura~ services.

Use Docker Compose to start the ~mongo_data_connector~, ~redis~ and
~hasura~ services.

#+begin_src bash :eval never-export :exports code :session scratch :results none
  docker compose up -d mongo_data_connector redis hasura
#+end_src

- What did this do? ::
  This step

** Step 17:  Open the Hasura Console and log in.

Open a browser to the Hasura Console.

#+begin_src bash :eval never-export :exports code :session scratch :results none
  xdg-open http://localhost:8081	# or your own port
#+end_src

- What did this do? ::
  This step just launched a web browser to the running instance of
  graphql-engine, which will cause the Hasura Console interface to
  appear.

* Part B:  In Hasura Console

** Step 1:  Add the postgres database and track its tables.

Use Hasura Console as illustrated here to add the ~postgres~ database
and track its tables. 

The database url is: ~postgres://postgres:postgres@postgres/chinook~.

Use Hasura Console as illustrated here to track /some/ of the
~postgres~ tables:

- Genre
- MediaType
- Playlist
- PlaylistTrack
- Customer
- Invoice
- InvoiceLine

Do not track these tables:

- Artist
- Album
- Track

The reason not to track these tables in the ~postgres~ database is
that these data will instead be brought in from the ~mongo~ database.

[[https://github.com/user-attachments/assets/77424ec0-e1ed-4241-92e8-7ed3ea5ba261][2024-07-31_10-53-36.webm]]

- What did this do? ::
  This step used Hasura Console to edit the Hasura metadata in order
  to add the ~postgres~ database (itself a Docker Compose service) as
  a data source.  It also "tracked" these tables, which means to add
  them to the GraphQL API.

** Step 2:  Add the mongo database and track the mongo collections

Use Hasura Console as illustrated here to add the ~mongo~ database.

The database url is:  ~mongodb://mongo:mongo@mongo:27017~

The database is:  ~admin~

Use Hasura Console as illustrated here to track the ~mongo~
collections.

Note that because MongoDB is a document database and can hold data
without a schema, an extra step is involved to choose the type for the
GraphQL schema.  A sample document from the MongoDB collection is
taken and used to generate corresponding Hasura Logical Models.  To do
this, run these commands and copy the output into Hasura Console when
track the collections.

#+begin_src bash :eval never-export :exports both :session scratch :results output
docker exec scratch-mongo-1 mongosh --quiet -u mongo -p mongo --eval "EJSON.stringify(db.postgres.Artist.findOne())" admin
docker exec scratch-mongo-1 mongosh --quiet -u mongo -p mongo --eval "EJSON.stringify(db.postgres.Album.findOne())" admin
docker exec scratch-mongo-1 mongosh --quiet -u mongo -p mongo --eval "EJSON.stringify(db.postgres.Track.findOne())" admin
#+end_src

[[https://github.com/user-attachments/assets/9b2c7c46-d7e3-41ef-aa81-c39f77feaabc][2024-07-31_11-23-07.webm]]

- What did this do? ::
  This step used Hasura Console to edit the Hasura metadata in order to
  add the ~mongo~ database (also a Docker Compose service) as a data
  source. As discussed above, it also sampled the mongo collections in
  order to track its collections with suitable Logical Models.

** Step 3:  Try a sample query.

Use Hasura Console as illustrated here to try a sample GraphQL query
that traverses both data source, ~postgres~ and ~mongo~, via the
relationships that were established earlier.

#+begin_src graphql
query MyQuery {
  Artist(limit: 1) {
    Name
    albums(limit: 1) {
      Title
      tracks(limit: 1) {
        Name
        genre {
          Name
        }
        mediatype {
          Name
        }
        playlisttracks {
          PlaylistId
          Playlist {
            Name
          }
        }
      }
    }
  }
}
#+end_src

[[https://github.com/user-attachments/assets/fcb542bf-1338-49a0-b6c2-41f7674d458b][2024-07-31_11-58-04.webm]]

- What did this do? ::
  This used the API tab in Hasura Console, itself a GraphQL client, to
  access the GraphQL endpoint, and issue a sample query.

#  LocalWords:  throughs rf EOF healthcheck mongosh ctx msg attr uuid
#  LocalWords:  connectionId connectionCount conn js os linux runtime
#  LocalWords:  negotiatedCompressors cfe SHA principalName extraInfo
#  LocalWords:  authenticationDatabase aa af bef bc ef ccc ObjectId
#  LocalWords:  cda bb redis EE HGPORT backend configs dataconnector
#  LocalWords:  uri xdg webm EJSON MyQuery mediatype playlisttracks
#  LocalWords:  PlaylistId
